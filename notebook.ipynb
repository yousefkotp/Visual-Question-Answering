{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm --user\n",
    "%pip install git+https://github.com/openai/CLIP.git --user\n",
    "%pip install pandas --user\n",
    "%pip install wordcloud --user\n",
    "%pip install sklearn --user\n",
    "%pip install scikit-learn --user\n",
    "%pip install Levenshtein --user\n",
    "%pip install cuda-python --user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import Levenshtein as lev\n",
    "\n",
    "import modelArchitecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = 'vizwiz'\n",
    "ANNOTATIONS = INPUT_PATH + '/Annotations/Annotations'\n",
    "TRAIN_PATH = INPUT_PATH + '/train/train'\n",
    "VALIDATION_PATH = INPUT_PATH + '/val/val'\n",
    "ANNOTATIONS_TRAIN_PATH = ANNOTATIONS + '/train.json'\n",
    "ANNOTATIONS_VAL_PATH = ANNOTATIONS + '/val.json'\n",
    "OUTPUT_PATH = ''\n",
    "ANSWER_SPACE = 0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    df = pd.read_json(path)\n",
    "    df = df[['image', 'question', 'answers', 'answer_type', 'answerable']]\n",
    "    return df\n",
    "\n",
    "def save_dataframe(dataframe, path, type = 'train'):\n",
    "\n",
    "    path+= 'dataframes' if path =='' else '/dataframes'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    if type == 'train':\n",
    "        dataframe.to_csv(path + '/train.csv', index=False)\n",
    "    elif type == 'val':\n",
    "        dataframe.to_csv(path + '/val.csv', index=False)\n",
    "    else:\n",
    "        dataframe.to_csv(path + '/test.csv', index=False)\n",
    "\n",
    "def split_train_test(dataframe, test_size = 0.05):\n",
    "    train, test = train_test_split(dataframe, test_size=test_size, random_state=42, stratify=dataframe[['answer_type', 'answerable']])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def plot_histogram(dataframe, column):\n",
    "    plt.hist(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pie(dataframe, column):\n",
    "    value_counts = dataframe[column].value_counts()\n",
    "    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_boxplot(dataframe, column):\n",
    "    plt.boxplot(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wordcloud(dataframe, column):\n",
    "\n",
    "    text = \" \".join([word for word in dataframe[column]])\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    min_font_size = 10).generate(text) \n",
    "    \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "def explore_dataframe(dataframe):\n",
    "\n",
    "    # let's see the distribution of the answer types\n",
    "    plot_pie(dataframe, 'answer_type')\n",
    "\n",
    "    # let's see the distribution of the answerable column\n",
    "    plot_pie(dataframe, 'answerable')\n",
    "\n",
    "    # let's plot histograms for the answerable column\n",
    "    plot_histogram(dataframe, 'answerable')\n",
    "\n",
    "    # let's see the distribution of the question column\n",
    "    plot_wordcloud(dataframe, 'question')\n",
    "    \n",
    "\n",
    "def get_number_of_distinct_answers(dataframe):\n",
    "    # Create a set to count the number of unique answers\n",
    "    unique_answers_set = set()\n",
    "\n",
    "    for row in dataframe['answers']:\n",
    "        for answer_map in row:\n",
    "            unique_answers_set.add(answer_map['answer'])\n",
    "    return len(unique_answers_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_dataframe(ANNOTATIONS_TRAIN_PATH)\n",
    "validation_df = read_dataframe(ANNOTATIONS_VAL_PATH)\n",
    "train_df, test_df = split_train_test(train_df, test_size=0.05)\n",
    "ANSWER_SPACE = get_number_of_distinct_answers(train_df)\n",
    "print(\"Number of distinct answers: \", ANSWER_SPACE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_dataframe(train_df)\n",
    "print(\"Number of distinct answers: \", get_number_of_distinct_answers(train_df))\n",
    "print(\"Number of samples in train: \", len(train_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_dataframe(validation_df)\n",
    "print(\"Number of distinct answers: \", get_number_of_distinct_answers(validation_df))\n",
    "print(\"Number of samples in validation set: \", len(validation_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_dataframe(test_df)\n",
    "print(\"Number of distinct answers: \", get_number_of_distinct_answers(test_df))\n",
    "print(\"Number of samples in test: \", len(test_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizWizDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_path = None):\n",
    "\n",
    "        super(VizWizDataset, self).__init__()\n",
    "        self.answer_counter = Counter()\n",
    "        self.dataframe = dataframe\n",
    "        self.image_path = image_path\n",
    "       \n",
    "        self.build_answer_counter()\n",
    "        self.build_answer_vocab()\n",
    "        # the columns of the dataframe are: image, question, answer, answer_type, answerable\n",
    "        print(\"Number of distinct answers: \", len(self.get_answer_vocab()))\n",
    "        print(self.dataframe.head())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataframe.iloc[index]\n",
    "    \n",
    "    def build_answer_vocab(self):\n",
    "        # This function assigns answer for each row in the dataframe and builds the answer vocab\n",
    "\n",
    "        # Building answer vocab follow this policy:\n",
    "        # for each 10 answers for a question, we choose the most frequent answer\n",
    "        # if there is a tie, we choose the most common one in the whole dataset\n",
    "        # if there is a tie, we choose the pairwise Levenshtein distance is used to find the answer that is most representative to all others.\n",
    "        \n",
    "        \n",
    "        # let's copy the original dataframe\n",
    "        copied_dataframe = self.dataframe.copy()\n",
    "        copied_dataframe.drop(columns=['answers'], inplace=True)\n",
    "\n",
    "        # add extra column named 'answer'\n",
    "        copied_dataframe['answer'] = None\n",
    "\n",
    "        for row, index in self.dataframe.iterrows():\n",
    "\n",
    "            intermediate_counter = Counter()\n",
    "            for answer_map in row:\n",
    "                answer = answer_map['answer']\n",
    "                intermediate_counter.update([answer])\n",
    "\n",
    "            # let's see the top elements in the answers_counter to check if there is a tie\n",
    "            top_elements = intermediate_counter.most_common(1)\n",
    "            if len(top_elements) == 1:\n",
    "                copied_dataframe.at[index, 'answer'] = top_elements[0][0]\n",
    "            else:\n",
    "                # let's see who is the most common answer in the whole dataset\n",
    "                top_elements = self.answer_counter.most_common(1)\n",
    "                if len(top_elements) == 1:\n",
    "                    copied_dataframe.at[index, 'answer'] = top_elements[0][0]\n",
    "                else:\n",
    "                    # let's get the minimum levenshtein distance which we have built before\n",
    "                    current_min = np.inf\n",
    "                    current_answer = None\n",
    "                    # now let's build levenshtein distance between the answers in top_elements and each others and select the minimum\n",
    "                    for answer in top_elements:\n",
    "                        total_distance = 0\n",
    "                        for answer2 in top_elements:\n",
    "                            if answer != answer2:\n",
    "                                lev_distance = lev.distance(answer[0], answer2[0])\n",
    "                                total_distance += lev_distance\n",
    "                        if total_distance < current_min:\n",
    "                            current_min = total_distance\n",
    "                            current_answer = answer[0]\n",
    "                    copied_dataframe.at[index, 'answer'] = current_answer\n",
    "\n",
    "        # now let's replace the original dataframe with the new one\n",
    "        self.dataframe = copied_dataframe\n",
    "        return\n",
    "\n",
    "    def build_answer_counter(self):\n",
    "        for row in self.dataframe['answers']:\n",
    "            for answer_map in row:\n",
    "                self.answer_counter.update([answer_map['answer']])\n",
    "    \n",
    "    def get_answer_vocab(self):\n",
    "        return self.dataframe['answer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = VizWizDataset(train_df)\n",
    "validation_dataset = VizWizDataset(validation_df)\n",
    "test_dataset = VizWizDataset(test_df)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"RN50x64\"\n",
    "NUM_EPOCHS = 10\n",
    "LR = 0.0001\n",
    "NUM_CLASSES = len(training_dataset.get_answer_vocab())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAVE_PATH = OUTPUT_PATH\n",
    "SAVE_EVERY = 1\n",
    "\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "model = modelArchitecture.VQAModel(num_classes=NUM_CLASSES, device= DEVICE, hidden_size=512, model_name=MODEL_NAME).to(DEVICE)\n",
    "model.print_CLIP_model()\n",
    "\n",
    "# let's define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# let's define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "model.train_model(training_dataloader, validation_dataloader, loss_function, optimizer, epochs=NUM_EPOCHS, save_path=SAVE_PATH, save_every=SAVE_EVERY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your own image !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is the color of the shirt?\"\n",
    "IMAGE_PATH = \"\"\n",
    "model.test_model(image_path = IMAGE_PATH, question = QUESTION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
